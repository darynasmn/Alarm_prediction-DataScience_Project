{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9e642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3757201255.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import pytz\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a1dfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\4131587841.py:7: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19658bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "import UTILS as U\n",
    "from UTILS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611c89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"0_isw_html\"\n",
    "INPUT_DATA_FOLDER = 'SHIIIIIIIIIIIIIIIIT'\n",
    "OUTPUT_FOLDER_CSV = 'info_for_prediction'\n",
    "OUTPUT_DATA_FILE_CSV = '0_predicted_isw'\n",
    "INPUT_FOLDER_ML = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6a6379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://understandingwar.org/backgrounder/russian-offensive-campaign-assessment-april-25-2023\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_FOLDER = \"SHIIIIIIIIIIIIIIIIT/0_isw_html\"\n",
    "BASA_URL = \"https://understandingwar.org/backgrounder/russian-offensive-campaign-assessment\"\n",
    "\n",
    "def save_page(url, file_name):\n",
    "    temp_url = url\n",
    "    temp_url = temp_url.split('-') \n",
    "    if temp_url[-2] < '10':\n",
    "        url = url[:len(url)-5]\n",
    "        if 'january' in url:\n",
    "            url = url.replace('0','') + '-2023'\n",
    "        else:\n",
    "            url = url.replace('0','')\n",
    "    print(url)\n",
    "    page = requests.get(url)\n",
    "    url_name = url.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "    with open(f\"{OUTPUT_FOLDER}/{file_name}__{url_name}.html\", 'wb+') as f:\n",
    "        f.write(page.content)\n",
    "\n",
    "def get_prev_date(what):\n",
    "    mnths = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\",\"september\", \"october\", \n",
    "             \"november\", \"december\"]\n",
    "    base = datetime.strptime(datetime.now().strftime(\"%Y-%m-%d\"), \"%Y-%m-%d\") - timedelta(days=1)\n",
    "    if what == \"m\":\n",
    "        return mnths[int(base.strftime(\"%m\"))-1]\n",
    "    elif what == \"d\":\n",
    "        return base.strftime(\"%d\")\n",
    "    elif what == \"year\":\n",
    "        return base.strftime(\"%Y\")\n",
    "\n",
    "month = get_prev_date(\"m\")\n",
    "day = get_prev_date(\"d\")\n",
    "year = get_prev_date(\"year\")\n",
    "url = f\"{BASA_URL}-{month}-{day}-{year}\"\n",
    "file_name = f\"{month}_{day}_{year}\"\n",
    "import os\n",
    "\n",
    "folder_path = f\"isw_data/0_isw_html\"\n",
    "file_name_2 =  file_name+'__russian_offensive_campaign_assessment_'+file_name+'.html'\n",
    "\n",
    "if not os.path.isfile(os.path.join(folder_path, file_name_2)):\n",
    "    save_page(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566f6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FOLDER = 'SHIIIIIIIIIIIIIIIIT/0_isw_html'\n",
    "by_dates = glob.glob(f'{OUTPUT_FOLDER}/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e4066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "for i in by_dates:\n",
    "    d={}\n",
    "    \n",
    "    file_name = i.split('\\\\')[-1].split('__')\n",
    "    date = datetime.strptime(file_name[0], '%B_%d_%Y')\n",
    "    url = file_name[1].split('.')[0]\n",
    "    \n",
    "    with open(i, 'r', encoding=\"utf-8\", errors='namereplace') as cfile:\n",
    "        parsed_html = BeautifulSoup(cfile, 'html.parser')\n",
    "        try:\n",
    "            title = parsed_html.head.find('title').text\n",
    "        except AttributeError:\n",
    "            title = \"\"\n",
    "        try:\n",
    "            link = parsed_html.head.find('link', attrs={'rel':\"canonical\"}, href = True).attrs[\"href\"]\n",
    "        except (AttributeError, KeyError):\n",
    "            link = \"\"\n",
    "        try:\n",
    "            text_title = parsed_html.body.find('h1', attrs={'id':'page-title'}).text\n",
    "        except AttributeError:\n",
    "            text_title = \"\"\n",
    "        try:\n",
    "            text_main = parsed_html.body.find('div', attrs={'class':'field field-name-body field-type-text-with-summary field-label-hidden'}).decode_contents(formatter=\"html\")\n",
    "        except AttributeError:\n",
    "            text_main = \"\"\n",
    "       \n",
    "        dictionary = {\n",
    "            'date':date,\n",
    "            'short_url':url,\n",
    "            'title':title,\n",
    "            'text_title':text_title,\n",
    "            'full_url':link,\n",
    "            'main_html':text_main\n",
    "        }\n",
    "        \n",
    "        all_data.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6e03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c04e8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>short_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text_title</th>\n",
       "      <th>full_url</th>\n",
       "      <th>main_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-25</td>\n",
       "      <td>russian_offensive_campaign_assessment_april_25...</td>\n",
       "      <td>Russian Offensive Campaign Assessment, April 2...</td>\n",
       "      <td>Russian Offensive Campaign Assessment, April 2...</td>\n",
       "      <td>/backgrounder/russian-offensive-campaign-asses...</td>\n",
       "      <td>&lt;div class=\"field-items\"&gt;&lt;div class=\"field-ite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                          short_url   \n",
       "0 2023-04-25  russian_offensive_campaign_assessment_april_25...  \\\n",
       "\n",
       "                                               title   \n",
       "0  Russian Offensive Campaign Assessment, April 2...  \\\n",
       "\n",
       "                                          text_title   \n",
       "0  Russian Offensive Campaign Assessment, April 2...  \\\n",
       "\n",
       "                                            full_url   \n",
       "0  /backgrounder/russian-offensive-campaign-asses...  \\\n",
       "\n",
       "                                           main_html  \n",
       "0  <div class=\"field-items\"><div class=\"field-ite...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bfb9325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daryna Semenets\\python_arbeiten\\.venv\\Scripts\\Project\\utils\\UTILS.py:124: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 124 of the file C:\\Users\\Daryna Semenets\\python_arbeiten\\.venv\\Scripts\\Project\\utils\\UTILS.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  parsed_html = BeautifulSoup(page_html_text)\n"
     ]
    }
   ],
   "source": [
    "from utils import UTILS\n",
    "df['main_html_v2'] = df['main_html'].apply(lambda x: UTILS.remove_names_and_dates(x, BeautifulSoup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0be3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '\\[(\\d+)\\]'\n",
    "\n",
    "df['main_html_v3'] = df['main_html_v2'].apply(lambda x: re.sub(pattern, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b2d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_html_v4'] = df['main_html_v3'].apply(lambda x: BeautifulSoup(x).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b7c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_html_v5'] = df['main_html_v4'].apply(lambda x: re.sub(r'http(\\S+.*\\s)','',x))\n",
    "df['main_html_v6'] = df['main_html_v5'].apply(lambda x: re.sub(r'(o2022|o2023|2022|2023)','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5636fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_html_v7'] = df['main_html_v6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38e0b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_clickHere(df):\n",
    "    divider = \"in this report.\"\n",
    "    for i in range(df.shape[0]):\n",
    "        if divider in df.loc[i,'main_html_v6']:\n",
    "            temp_str = df.loc[i,'main_html_v6']\n",
    "            temp_list = temp_str.split(divider)\n",
    "            df.loc[i,'main_html_v7'] = temp_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd97b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_clickHere(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3861ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_clickHere(df):\n",
    "    divider = \"invasion of Ukraine.\"\n",
    "    for i in range(df.shape[0]):\n",
    "        if divider in df.loc[i,'main_html_v7']:\n",
    "            temp_str = df.loc[i,'main_html_v7']\n",
    "            temp_list = temp_str.split(divider)\n",
    "            df.loc[i,'main_html_v7'] = temp_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c8bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_clickHere(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00bb58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_html'] = df['main_html'].apply(lambda x: BeautifulSoup(x).text)\n",
    "df2 = df.drop(['main_html_v2','main_html_v3','main_html_v4',\n",
    "               'main_html_v5', 'main_html_v6'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79647a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, word_root_algo= \"lemm\"):\n",
    "    from UTILS import remove_one_letter_word,remove_url_string, convert_lower_case, remove_punctuation, remove_apostrophe, remove_stop_words, conver_numbers, stemming, lemmatizing\n",
    "    data = UTILS.remove_one_letter_word(data)\n",
    "    data = UTILS.remove_url_string(data)\n",
    "    data = UTILS.convert_lower_case(data)\n",
    "    data = UTILS.remove_punctuation(data)\n",
    "    data = UTILS.remove_apostrophe(data)\n",
    "    data = UTILS.remove_stop_words(data)\n",
    "    data = UTILS.conver_numbers(data)\n",
    "    data = UTILS.stemming(data)\n",
    "    data = UTILS.remove_punctuation(data)\n",
    "    data = UTILS.conver_numbers(data)\n",
    "    \n",
    "    if word_root_algo == \"lemm\":\n",
    "        data = UTILS.lemmatizing(data)\n",
    "    else:\n",
    "        data = UTILS.stemming(data)\n",
    "        \n",
    "    data = UTILS.remove_punctuation(data)\n",
    "    data = UTILS.remove_stop_words(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443b3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['report_text_lemm']= df2['main_html_v7'].apply(lambda x: preprocess(x,'lemm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d59153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(f'{OUTPUT_FOLDER_CSV}/{OUTPUT_DATA_FILE_CSV}', sep =';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b5d0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df2['report_text_lemm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "722bb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f05749",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/pred_count_vectorizer_v1.pkl', 'wb') as handle:\n",
    "    pickle.dump(cv ,handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1de876ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer =  TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2859a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/pred_tfidf_transformer_v1.pkl', 'wb') as handle:\n",
    "    pickle.dump(tfidf_transformer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b96d2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index = cv.get_feature_names_out(), columns = ['idf_weights'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28651cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 828)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vector = tfidf_transformer.transform(word_count_vector)\n",
    "tf_idf_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cce770f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pickle.load(open('models/pred_tfidf_transformer_v1.pkl', 'rb'))\n",
    "cv = pickle.load(open('models/pred_count_vectorizer_v1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "649fe3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver_doc_to_vector(doc):\n",
    "    feature_names = cv.get_feature_names_out()\n",
    "    top_n = 100\n",
    "    tf_idf_vector = tfidf.transform(cv.transform([doc]))\n",
    "    \n",
    "    sorted_items = tf.sort_coo(tf_idf_vector.tocoo())\n",
    "    \n",
    "    keywords = tf.extract_topn_from_vector(feature_names, sorted_items,top_n)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc40d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tf\n",
    "df2['keywords'] = df2['report_text_lemm'].apply(lambda x: conver_doc_to_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ec85a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2['report_date']=df2['date']\n",
    "df2['date_tmrw'] = df2['report_date'].apply(lambda x: x+timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f85baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "isw_df = df2[[\"report_date\", \"date_tmrw\", \"keywords\", \"main_html_v7\", \"report_text_lemm\"]].copy().add_prefix('isw_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79319e2",
   "metadata": {},
   "source": [
    "# here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baf2793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "import pytz\n",
    "import os\n",
    "import scipy\n",
    "import pandas as pd             \n",
    "\n",
    "API_KEY = \"V8J7UYNGWSQ65E3KGUDZ4YKF4\"\n",
    "\n",
    "DIR_REGIONS = 'Homework3/raw_csv/regions.csv'\n",
    "SAVED_FORCASTS = 'SHIIIIIIIIIIIIIIIIT/saved_weather'\n",
    "\n",
    "\n",
    "df_regions = pd.read_csv(DIR_REGIONS)\n",
    "\n",
    "def save_file(data,city, date):\n",
    "    data_object = json.dumps(data)\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(f\"{SAVED_FORCASTS}/{city}_{date}.json\",\"w\")\n",
    "\n",
    "    # write json object to file\n",
    "    f.write(data_object)\n",
    "\n",
    "    # close file\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    f = open(path)\n",
    "  \n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "  \n",
    "  \n",
    "    # Closing file\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def get_weather(city, date):\n",
    "    \n",
    "    path = f\"{SAVED_FORCASTS}/{city}_{date}.json\"\n",
    "    if (os.path.exists(path)):\n",
    "        jsonData = read_file(path)\n",
    "        return jsonData\n",
    "    location = f\"{city},Ukraine\"\n",
    "    url = f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{location}/{date}?key={API_KEY}&include=hours&unitGroup=metric&contentType=json'\n",
    "    try: \n",
    "      ResultBytes = urllib.request.urlopen(url)\n",
    "  \n",
    "      # Parse the results as JSON\n",
    "      jsonData = json.load(ResultBytes)\n",
    "\n",
    "        \n",
    "    except urllib.error.HTTPError  as e:\n",
    "      ErrorInfo= e.read().decode() \n",
    "      print('Error code: ', e.code, ErrorInfo)\n",
    "      sys.exit()\n",
    "    except  urllib.error.URLError as e:\n",
    "      ErrorInfo= e.read().decode() \n",
    "      print('Error code: ', e.code,ErrorInfo)\n",
    "      sys.exit()\n",
    "    save_file(jsonData,city, date)\n",
    "    return jsonData\n",
    "\n",
    "\n",
    "\n",
    "def get_next_date(date):\n",
    "    return (date+datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def get_df_weather(jsonData):\n",
    "    df_data_day = pd.DataFrame(jsonData['days'])\n",
    "    df_data_day = df_data_day[df_data_day.columns[0:33]].add_prefix('day_')\n",
    "    hours_forecast=jsonData['days'][0]['hours']\n",
    "    df_weather_hours = pd.DataFrame(hours_forecast).add_prefix('hour_')\n",
    "    df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
    "    df_weather_hours['key'] = 1\n",
    "    df_data_day['key'] = 1\n",
    "    df_weather_final = pd.merge(df_data_day,df_weather_hours, on='key')\n",
    "    return df_weather_final\n",
    "\n",
    "\n",
    "def get_weather_for_12_hours(city,date):\n",
    "    jsonData = get_weather(city, date.strftime(\"%Y-%m-%d\"))\n",
    "    current_hour = int(date.strftime(\"%H\"))\n",
    "    weather_all_data_day1 = get_df_weather(jsonData)\n",
    "    hours_needed = (weather_all_data_day1['hour_int']>=current_hour)&(weather_all_data_day1['hour_int']<=(current_hour+12))\n",
    "    weather_all_data_day1=weather_all_data_day1[hours_needed]\n",
    "    df_weather_final = weather_all_data_day1\n",
    "    hours_left=12-weather_all_data_day1.shape[0]\n",
    "    if(hours_left>0):\n",
    "        jsonData = get_weather(city, get_next_date(date))\n",
    "        weather_all_data_day2 = get_df_weather(jsonData)\n",
    "        hours_needed_2 = ((weather_all_data_day2['hour_int']<=hours_left))\n",
    "        weather_all_data_day2=weather_all_data_day2[hours_needed_2]\n",
    "        df_weather_final = pd.concat([weather_all_data_day1, weather_all_data_day2], axis=0)\n",
    "    df_weather_final['city']=city\n",
    "    df_final = pd.merge(df_weather_final,df_regions,left_on=\"city\",right_on=\"center_city_en\")\n",
    "\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83b3e5",
   "metadata": {},
   "source": [
    "# УВАГА\n",
    "тут векторайзер той самий що для тренування моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b5f62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = pickle.load(open (\"models/tfidf_transformer_v1.pkl\", \"rb\"))\n",
    "cv_model = pickle.load(open (\"models/count_vectorizer_v1.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "555ed394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_vector_model = cv_model.transform(isw_df['isw_report_text_lemm'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a28d3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF_IDF_MODEL = tf_idf_model.transform(cv_vector_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3cfca5",
   "metadata": {},
   "source": [
    "# MAIN PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2be808da",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_MODELS = 'training_models'\n",
    "name = 'random_forest'\n",
    "version = '1.1.pkl'\n",
    "name_model = f'{name}__{version}'\n",
    "model_4 = pickle.load(open (f\"{INPUT_MODELS}/{name_model}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fd329eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n",
      "C:\\Users\\Daryna Semenets\\AppData\\Local\\Temp\\ipykernel_6260\\3537637949.py:80: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_weather_hours['hour_int']=pd.to_datetime(df_weather_hours['hour_datetime']).dt.hour\n"
     ]
    }
   ],
   "source": [
    "cities = ['Vinnytsia','Simferopol','Lutsk','Dnipro','Donetsk','Zhytomyr','Uzhgorod','Zaporozhye','Ivano-Frankivsk','Kyiv','Kropyvnytskyi',\n",
    "         'Luhansk','Lviv','Mykolaiv','Odesa','Poltava','Rivne','Sumy','Ternopil','Kharkiv','Kherson','Khmelnytskyi',\n",
    "         'Cherkasy','Chernivtsi','Chernihiv']\n",
    "date = datetime.datetime.now(pytz.timezone('Europe/Kyiv'))\n",
    "result = {}\n",
    "for city in cities:\n",
    "    df_weather_final = get_weather_for_12_hours(city,date)\n",
    "    df_weather_final['key']=1\n",
    "    isw_df['key']=1\n",
    "    df_all = df_weather_final.merge(isw_df, how = 'left', left_on = 'key', right_on = 'key')\n",
    "    to_drop=['key','isw_report_date','isw_date_tmrw','isw_keywords','isw_main_html_v7','isw_report_text_lemm']\n",
    "    df_weather_matrix_v1 = df_all.drop(to_drop, axis = 1)\n",
    "    df_weather_matrix_v1= df_weather_matrix_v1[['day_tempmax', 'day_tempmin', 'day_temp', 'day_dew', 'day_humidity',\n",
    "           'day_precip', 'day_precipcover', 'day_solarradiation',\n",
    "           'day_solarenergy', 'day_uvindex', 'hour_temp', 'hour_humidity',\n",
    "           'hour_dew', 'hour_precip', 'hour_precipprob', 'hour_snow',\n",
    "           'hour_snowdepth', 'hour_windgust', 'hour_windspeed', 'hour_winddir',\n",
    "           'hour_pressure', 'hour_visibility', 'hour_cloudcover',\n",
    "           'hour_solarradiation', 'hour_uvindex', 'hour_severerisk','region_id']]\n",
    "    cv_vector_model = cv_model.transform(df_all['isw_report_text_lemm'].values.astype('U'))\n",
    "    TF_IDF_MODEL = tf_idf_model.transform(cv_vector_model)\n",
    "    df_weather_matrix_v1_csr = scipy.sparse.csr_matrix(df_weather_matrix_v1.values)\n",
    "    df_all_data_csr = scipy.sparse.hstack((df_weather_matrix_v1_csr, TF_IDF_MODEL), format='csr')\n",
    "    predicted = model_4.predict(df_all_data_csr)\n",
    "    current_time = datetime.datetime.now(pytz.timezone('Europe/Kyiv'))\n",
    "    hours = []\n",
    "    for i in range(12):\n",
    "        hour = date + datetime.timedelta(hours=i)\n",
    "        hour_rounded = hour.replace(minute=0, second=0, microsecond=0)\n",
    "        hours.append(hour_rounded.strftime('%H:%M'))\n",
    "\n",
    "    result[city] = dict(zip(hours, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c960fb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Vinnytsia': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Simferopol': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Lutsk': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Dnipro': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Donetsk': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Zhytomyr': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Uzhgorod': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Zaporozhye': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Ivano-Frankivsk': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Kyiv': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Kropyvnytskyi': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Luhansk': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Lviv': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Mykolaiv': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Odesa': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Poltava': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Rivne': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Sumy': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Ternopil': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Kharkiv': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Kherson': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Khmelnytskyi': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Cherkasy': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Chernivtsi': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False},\n",
       " 'Chernihiv': {'20:00': False,\n",
       "  '21:00': False,\n",
       "  '22:00': False,\n",
       "  '23:00': False,\n",
       "  '00:00': False,\n",
       "  '01:00': False,\n",
       "  '02:00': False,\n",
       "  '03:00': False,\n",
       "  '04:00': False,\n",
       "  '05:00': False,\n",
       "  '06:00': False,\n",
       "  '07:00': False}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac40830",
   "metadata": {},
   "source": [
    "### MODEL PART"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
